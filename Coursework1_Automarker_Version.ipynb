{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coursework 1 - revised and can be used with automarker\n",
    "\n",
    "In this coursework you will be aiming to complete two classification tasks. One of the classification tasks is related to image classification and the other relates to text classification.\n",
    "\n",
    "The specific tasks and the marking for the various tasks are provided in the notebook. Each task is expected to be accompanied by a lab-report. Each task can have a concise lab report that is maximum of one page in an A4 size. You will be expected to submit your Jupyter Notebook and all lab reports as a single PDF file. You could have additional functions implemented that you require for carrying out each task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 1\n",
    "\n",
    "In this task, you are provided with three classes of images, cars, bikes and people in real world settings. You are provided with code for obtaining features for these images (specifically histogram of gradients (HoG) features). You need to implement a boosting based classifier that can be used to classify the images. \n",
    "\n",
    "This task is worth 30 points out of 100 points. \n",
    "Implementing a working boosting based classifier and validating it by cross-validation on the training set will be evaluated for 15 out of 30 points. 10 points are based on the evaluation carried out on a separate test dataset that will be done at the time of evaluation. Finally 5 points are reserved for analysis of this part of the task and presenting it well in a lab report. \n",
    "\n",
    "Note that the boosting classifier you implement can include decision trees from your previous ML1 coursework or can be a decision stump. Use the image_dataset directory provided with the assignment and save it in the same directory as the Python notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write your  Image feature extraction code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2 as cv\n",
    "import glob\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import sklearn.metrics as metrics\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import matplotlib.pyplot as plt    \n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "plt.rcParams['figure.figsize'] = [10, 6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for normalizing data\n",
    "def normalize(df, mean_method = True ):\n",
    "    if mean_method:\n",
    "        # mean normalization\n",
    "        normalized_df = (df - df.mean()) / df.std()\n",
    "        a, b = df.std(), df.mean()\n",
    "    else:\n",
    "        # min max normalization\n",
    "        normalized_df = (df - df.min()) / (df.max() - df.min())\n",
    "        a, b = df.min(), df.max()\n",
    "    return normalized_df, a, b\n",
    "\n",
    "def unnormalize(df, a, b, mean_method = True):\n",
    "    if mean_method:\n",
    "        # mean normalization\n",
    "        unnormalized_df = df * a + b\n",
    "    else:\n",
    "        # min max normalization\n",
    "        unnormalized_df = (df * (b - a)) + a\n",
    "    return unnormalized_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augmentation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def HOG_augment(df, augment = True, n = 2):\n",
    "    \n",
    "    from keras.preprocessing.image import ImageDataGenerator, img_to_array, array_to_img\n",
    "    \n",
    "    datagen = ImageDataGenerator( \n",
    "            rotation_range = 40, \n",
    "            shear_range = 0.2, \n",
    "            zoom_range = 0.2, \n",
    "            horizontal_flip = True, \n",
    "            brightness_range = (0.5, 1.5))\n",
    "    \n",
    "    hog = cv.HOGDescriptor()\n",
    "    \n",
    "    x = []\n",
    "    y = []\n",
    "    \n",
    "    \n",
    "    \n",
    "    for image, target in df.values:\n",
    "        \n",
    "        # get hog features for OG images\n",
    "        x.append(hog.compute(image))\n",
    "        y.append(target)\n",
    "        \n",
    "        x_array = img_to_array(image)\n",
    "        x_array = x_array.reshape((1,) + x_array.shape)\n",
    "        \n",
    "        if augment:\n",
    "            i = 0\n",
    "            for batch in datagen.flow(x_array):\n",
    "\n",
    "                b = np.asarray(array_to_img(np.squeeze(batch)))\n",
    "                h = hog.compute(b)\n",
    "\n",
    "                # save hog features and class for augmented images\n",
    "                x.append(h)\n",
    "                y.append(target)\n",
    "\n",
    "                i+=1\n",
    "                if i >= n:\n",
    "                    break\n",
    "                \n",
    "                \n",
    "    X = np.hstack(x).T \n",
    "    \n",
    "    df_augmented = pd.DataFrame(X)\n",
    "    df_augmented['y'] = y\n",
    "    \n",
    "    return df_augmented\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def reduce_features(df, num_features = 20):\n",
    "    from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "    from sklearn.decomposition import PCA\n",
    "    from tqdm import tqdm\n",
    "    \n",
    "    encoder = LabelEncoder()\n",
    "    \n",
    "    # split input and target \n",
    "    x, y = df.iloc[:,:-1], df.iloc[:,-1]\n",
    "    \n",
    "    for col in (x.columns[:]):\n",
    "        x[col] = encoder.fit_transform(x[col])\n",
    "    \n",
    "    # apply scaler to input features\n",
    "    scaler = StandardScaler()\n",
    "    x = scaler.fit_transform(x)\n",
    "\n",
    "    # transform features\n",
    "    pca = PCA()\n",
    "    pca.fit_transform(x)\n",
    "    pca_variance = pca.explained_variance_\n",
    "\n",
    "    # fit\n",
    "    pca2 = PCA(n_components = num_features, whiten = True)\n",
    "    pca2.fit(x)\n",
    "    pca_x = pca2.transform(x)\n",
    "\n",
    "    # recreate dataframe after PCA\n",
    "    df_pca = pd.DataFrame(pca_x)\n",
    "    df_pca['y'] = y.values\n",
    "    \n",
    "    return df_pca"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "def load_data(folder_name, \n",
    "                   include_augmented = True, \n",
    "                   Hog_features = False):\n",
    "    import os\n",
    "    \n",
    "    feature_len = 34020\n",
    "    hog = cv.HOGDescriptor()\n",
    "\n",
    "    y = []\n",
    "    X = []\n",
    "\n",
    "    c = 0\n",
    "    \n",
    "    # store images to visualize if necessary\n",
    "    image_dict = {}\n",
    "    \n",
    "    class_dict = {'bike'    : 1, \n",
    "                  'car'     : 2, \n",
    "                  'people'  : 3} \n",
    "    \n",
    "    for subdir, dirs, files in os.walk(folder_name):\n",
    "        for file in files:\n",
    "            if not include_augmented: \n",
    "                if 'aug' in file:\n",
    "                    file = ''\n",
    "            \n",
    "            # get file location from directory\n",
    "            file_location = subdir + os.path.sep + file\n",
    "            \n",
    "            try:\n",
    "                img = cv.imread(file_location)\n",
    "                image_dict.update({file:img})\n",
    "                h = hog.compute(img)\n",
    "                \n",
    "                if Hog_features:\n",
    "                    X.append(h)\n",
    "                else:\n",
    "                    X.append([img])\n",
    "                \n",
    "                if 'bike' in file:\n",
    "                    y.append(class_dict['bike'])\n",
    "                elif 'car' in file:\n",
    "                    y.append(class_dict['car'])\n",
    "                elif 'person' or 'people' in file:\n",
    "                    y.append(class_dict['people'])\n",
    "                else:\n",
    "                    y.append('error')\n",
    "            except:\n",
    "                print('Error when loading file : ', file)\n",
    "    if Hog_features:\n",
    "        X = np.concatenate(X, axis = 1).T\n",
    "    print('Data Loaded.')\n",
    "    return X, y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "def obtain_dataset_train(folder_name):\n",
    "    # import dependancies\n",
    "    import numpy as np\n",
    "    import cv2 as cv\n",
    "    import glob\n",
    "    import pandas as pd\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "    from sklearn import svm\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "    from sklearn.decomposition import PCA\n",
    "    import sklearn.metrics as metrics\n",
    "    import os\n",
    "    from tqdm import tqdm\n",
    "    import time\n",
    "    import matplotlib.pyplot as plt    \n",
    "    import seaborn as sns\n",
    "    \n",
    "    # load raw images w/o HOG features\n",
    "    x, y = load_data(folder_name, \n",
    "                   include_augmented = False, \n",
    "                   Hog_features = False)\n",
    "    df = pd.DataFrame(x)\n",
    "    df['y'] = y\n",
    "    \n",
    "    print('Original Shape: ', df.shape)\n",
    "    # augment and extract HOG from raw images\n",
    "    df = HOG_augment(df, augment = False, n = 5)\n",
    "    # dimensionality reduction w/ PCA\n",
    "    df = reduce_features(df, num_features = 150)\n",
    "    \n",
    "    print('Training Shape: ', df.shape)\n",
    "    # seperate input and target\n",
    "    X, y = df.iloc[:,:-1], df.iloc[:,-1].values\n",
    "    \n",
    "    # normalize input\n",
    "    X = normalize(X)[0]\n",
    "    \n",
    "    return (X,y) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Optional function for those who want to include pre-processing for train data in obtain dataset\n",
    "def obtain_dataset_test(folder_name_test):\n",
    "    # import dependancies\n",
    "    import numpy as np\n",
    "    import cv2 as cv\n",
    "    import glob\n",
    "    import pandas as pd\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "    from sklearn import svm\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "    from sklearn.decomposition import PCA\n",
    "    import sklearn.metrics as metrics\n",
    "    import os\n",
    "    from tqdm import tqdm\n",
    "    import time\n",
    "    import matplotlib.pyplot as plt    \n",
    "    import seaborn as sns\n",
    "    \n",
    "    # load raw test images\n",
    "    x, y = load_data(folder_name_test, \n",
    "                   include_augmented = False, \n",
    "                   Hog_features = True)\n",
    "    df = pd.DataFrame(x)\n",
    "    df['y'] = y\n",
    "    # PCA\n",
    "    df = reduce_features(df, num_features = 150)\n",
    "    \n",
    "    # seperate input and target\n",
    "    X, y = df.iloc[:,:-1], df.iloc[:,-1].values\n",
    "    \n",
    "    # normalize input\n",
    "    X = normalize(X)[0]\n",
    "    return (X, y) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Boosting classifier class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BoostingClassifier:\n",
    "    \n",
    "    def __init__(self,n_estimators = 100,\n",
    "                 max_depth = 5,\n",
    "                 num_features = 1000,\n",
    "                 start_feature = 200, \n",
    "                 sklearn = True):\n",
    "        \n",
    "        self.sklearn = sklearn\n",
    "        \n",
    "        self.n_est = n_estimators\n",
    "        self.depth = max_depth\n",
    "        \n",
    "        self.trees = []\n",
    "        self.tree_weights = []\n",
    "        \n",
    "        self.num_features = num_features\n",
    "        self.start = start_feature\n",
    "        \n",
    "    def update_weights(self, weights, y, y_dt, tree_weights):\n",
    "        \n",
    "        for i in range(weights.shape[0]):\n",
    "            if y[i] != y_dt[i]:\n",
    "                weights[i] = weights[i]*np.exp(tree_weights)\n",
    "        \n",
    "        weights /= np.sum(weights)\n",
    "        return weights\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \n",
    "        n_class = np.unique(y).shape[0]\n",
    "        \n",
    "        # init weights, value is aribitrary\n",
    "        weights = np.array([1/X.shape[0] for i in range(X.shape[0])])\n",
    "        weights = weights.reshape([-1,1])\n",
    "        \n",
    "        start = self.start \n",
    "        features = np.arange(start , start + self.num_features)   \n",
    "        \n",
    "        for i in range(self.n_est):\n",
    "            if self.sklearn == False:\n",
    "                # instantiate DT & fit to data\n",
    "                DT = Decision_Tree(X, y,\n",
    "                      num_features = self.num_features,\n",
    "                      max_depth = self.depth,\n",
    "                      start_feature = self.start)\n",
    "            elif self.sklearn == True:\n",
    "                DT = DecisionTreeClassifier(max_depth = self.depth)\n",
    "                DT.fit(X,y)\n",
    "            \n",
    "            # predict with trees\n",
    "            dt_pred = DT.predict(X)\n",
    "            \n",
    "            # update tree weights\n",
    "            error = np.sum(weights[np.where(y != dt_pred)])\n",
    "            self.tree_weights.append(np.log((1 - error) / error) \\\n",
    "                                     + np.log(n_class - 1))\n",
    "            \n",
    "            # update data weights\n",
    "            weights = self.update_weights(weights, \n",
    "                                          y, \n",
    "                                          dt_pred, \n",
    "                                          self.tree_weights[-1])\n",
    "            # store each tree\n",
    "            self.trees.append(DT)\n",
    "            \n",
    "    \n",
    "    def predict(self, X):\n",
    "        \n",
    "        n = X.shape[0]\n",
    "        \n",
    "        pred = np.array([tree.predict(X) for tree in self.trees]).T\n",
    "        \n",
    "        y_pred = []\n",
    "        for i in range(n):\n",
    "            \n",
    "            current = pred[i,:]\n",
    "            class_weights = {prediction : 0 for prediction in np.unique(current)}\n",
    "            for j, prediction in enumerate(current):\n",
    "                class_weights[prediction] += self.tree_weights[j]\n",
    "                \n",
    "            optimal_weight = max(class_weights, key = class_weights.get)\n",
    "            y_pred.append(optimal_weight)\n",
    "            \n",
    "        return np.array(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test function that will be called to evaluate your code. Separate train and test dataset will be provided\n",
    "\n",
    "Do not modify the code below. Please write your code above such that it can be evaluated by the function below. You can modify your code above such that you obtain the best performance through this function. We will also be evaluating the cross-validation performance with a set train and val split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_func_boosting_image(image_dataset_train, image_dataset_test):\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    (X_train, Y_train) = obtain_dataset_train(image_dataset_train)\n",
    "    (X_test, Y_test) = obtain_dataset_test(image_dataset_test)# optionally replace the two calls with a single call to obtain_dataset_train_test() function\n",
    "    bc = BoostingClassifier()\n",
    "    bc.fit(X_train, Y_train)\n",
    "    y_pred = bc.predict(X_test)\n",
    "    print('Accuracy: ', np.sum(y_pred == Y_test) / len(y_pred))\n",
    "    acc = accuracy_score(Y_test, y_pred)\n",
    "    print('Accuracy: ', acc)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 2\n",
    "\n",
    "In this task, you need to classify the above dataset using a Support Vector Machine (SVM).\n",
    "\n",
    "This task is worth 25 points out of 100 points. You are allowed to use existing library functions such as scikit-learn for obtaining the SVM. The main idea is to analyse the dataset using different kind of kernels. You are also supposed to write your own custom kernels. The marking will be 15 marks for analysing the dataset using various kernels including your own kernels, 5 points for the performance on the test dataset and 5 points for a lab-report that provides the analysis and comparisons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "class SVMClassifier:\n",
    "    \n",
    "    def __init__(self, kernel = 'rbf',\n",
    "                       reg_param = 1.0,\n",
    "                       degree = 3,\n",
    "                       max_iter = 1e5,\n",
    "                       tol = 1e-3):\n",
    "        \n",
    "        \n",
    "        if kernel == 'sigmoid':\n",
    "            k = self.sigmoid\n",
    "        elif kernel == 'linear':\n",
    "            k = self.linear\n",
    "        elif kernel == 'poly':\n",
    "            k = self.polynomial\n",
    "        elif kernel == 'gaussian':\n",
    "            k = self.gaussian\n",
    "        elif kernel == 'laplacian':\n",
    "            k = self.laplacian\n",
    "        elif kernel == 'log':\n",
    "            k = self.log\n",
    "        elif kernel == 'exponential':\n",
    "            k = self.exponential\n",
    "        elif kernel == 'rational':\n",
    "            k = self.rational\n",
    "        elif kernel == 'quadric':\n",
    "            k = self.quadric\n",
    "        elif kernel == 'rbf':\n",
    "            k = kernel\n",
    "        elif kernel == 'linear_sklearn':\n",
    "            k = 'linear'\n",
    "        elif kernel == 'poly_sklearn':\n",
    "            k = 'poly'\n",
    "        elif kernel == 'intersection':\n",
    "            k = self.Histo_intersection\n",
    "        else:\n",
    "            print('Using externel kernel.')\n",
    "            k = kernel\n",
    "            #return\n",
    "        \n",
    "        # instantial svm classifier\n",
    "        self.clf = svm.SVC(C = reg_param,\n",
    "              kernel = k,\n",
    "              degree = degree,\n",
    "              gamma = 'scale')\n",
    "        \n",
    "    def kernels_list(self):\n",
    "        return ['sigmoid', 'linear', 'poly', 'gaussian', 'laplacian', \n",
    "                'log', 'exponential', 'rational', 'quadric', 'rbf', \n",
    "                'linear_sklearn', 'poly_sklearn']\n",
    "    \n",
    "    def linear(self, x,y):\n",
    "        return np.inner(x,y)\n",
    "    \n",
    "    def sigmoid(self, x,y, alpha = 1):\n",
    "        return np.tanh(alpha*np.inner(x,y))\n",
    "    \n",
    "    def polynomial(self, x,y, coef = 1, p = 6):\n",
    "        return (np.inner(x,y) + coef) ** p\n",
    "        \n",
    "    def gaussian(self, U,V,sigma = 0.1):\n",
    "        def gaussianKernel(U,V,sigma = 0.1):\n",
    "            return np.exp(np.linalg.norm(U-V) ** 2 / (2*sigma**2))\n",
    "\n",
    "        G = np.zeros((U.shape[0], V.shape[0]))\n",
    "        for i in range(0,U.shape[0]):\n",
    "            for j in range(0,V.shape[0]):\n",
    "                G[i][j] = gaussianKernel(U[i],V[j],sigma)\n",
    "        return G    \n",
    "    \n",
    "    def laplacian(self, U,V,sigma = 0.1):\n",
    "        def LaplacianKernel(U,V,sigma = 0.1):\n",
    "            return np.exp(-np.linalg.norm(U-V) / sigma)\n",
    "        G = np.zeros((U.shape[0], V.shape[0]))\n",
    "        for i in range(0,U.shape[0]):\n",
    "            for j in range(0,V.shape[0]):\n",
    "                G[i][j] = LaplacianKernel(U[i],V[j],sigma)\n",
    "        return G\n",
    "    \n",
    "    def log(self, U,V):\n",
    "        def logKernel(U,V):\n",
    "            return -np.log(np.linalg.norm(U-V) + 1)\n",
    "        G = np.zeros((U.shape[0], V.shape[0]))\n",
    "        for i in range(0,U.shape[0]):\n",
    "            for j in range(0,V.shape[0]):\n",
    "                G[i][j] = logKernel(U[i],V[j])\n",
    "        return G\n",
    "    \n",
    "    def exponential(self, U,V,sigma = 0.1):\n",
    "        def expKernel(U,V,sigma = 0.1):\n",
    "            return - np.linalg.norm(U-V) /  (2*sigma**2) \n",
    "        G = np.zeros((U.shape[0], V.shape[0]))\n",
    "        for i in range(0,U.shape[0]):\n",
    "            for j in range(0,V.shape[0]):\n",
    "                G[i][j] = expKernel(U[i],V[j],sigma)\n",
    "        return G\n",
    "    \n",
    "    def rational(self, U,V,c = 100):\n",
    "        def RationalKernel(U,V,c = 100):\n",
    "            return 1 - (np.linalg.norm(U-V)**2 / (np.linalg.norm(U-V)**2 + c))\n",
    "        G = np.zeros((U.shape[0], V.shape[0]))\n",
    "        for i in range(0,U.shape[0]):\n",
    "            for j in range(0,V.shape[0]):\n",
    "                G[i][j] = RationalKernel(U[i],V[j],c)\n",
    "        return G\n",
    "    \n",
    "    def quadric(self, U,V,c = 100):\n",
    "        def quadricKernel(U,V,c = 100):\n",
    "            return np.sqrt(np.sum(np.power((U-V),2)) + c**2)\n",
    "        G = np.zeros((U.shape[0], V.shape[0]))\n",
    "        for i in range(0,U.shape[0]):\n",
    "            for j in range(0,V.shape[0]):\n",
    "                G[i][j] = quadricKernel(U[i],V[j],c)\n",
    "        return G\n",
    "    \n",
    "    def intersect(self,U,V,sigma = 0.1):\n",
    "        def Kernel(U,V,sigma = 0.1):\n",
    "            return  np.sum((min(U),  min(V)), axis = 0)\n",
    "        G = np.zeros((U.shape[0], V.shape[0]))\n",
    "        for i in range(0,U.shape[0]):\n",
    "            for j in range(0,V.shape[0]):\n",
    "                G[i][j] = Kernel(U[i],V[j],sigma)\n",
    "        return G\n",
    "    \n",
    "    def fit(self, X,y):\n",
    "        self.clf.fit(X,y)\n",
    "    \n",
    "    def fit_image(self, X,y):\n",
    "        self.clf.fit(X,y)\n",
    "    \n",
    "    def fit_text(self, X,y):\n",
    "        self.clf.fit(X,y)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        y_pred = self.clf.predict(X)\n",
    "        return y_pred\n",
    "    \n",
    "    def predict_image(self, X):\n",
    "        y_pred = self.clf.predict(X)\n",
    "        return y_pred\n",
    "    \n",
    "    def predict_text(self, X):\n",
    "        y_pred = self.clf.predict(X)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test function that will be called to evaluate your code. Separate train and test dataset will be provided\n",
    "\n",
    "Do not modify the code below. Please write your code above such that it can be evaluated by the function below. You can modify your code above such that you obtain the best performance through this function. We will also be evaluating the cross-validation performance with a set train and val split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_func_svm_image(image_dataset_train, image_dataset_test):\n",
    "    from sklearn.metrics import accuracy_score  \n",
    "    from sklearn.utils import check_X_y\n",
    "    (X_train, Y_train) = obtain_dataset_train(image_dataset_train)\n",
    "    (X_test, Y_test) = obtain_dataset_test(image_dataset_test) # optionally replace the two calls with a single call to obtain_dataset_train_test() function\n",
    "    sc = SVMClassifier()\n",
    "    sc.fit_image(X_train, Y_train)\n",
    "    y_pred = sc.predict_image(X_test)\n",
    "    print('Accuracy: ', np.sum(y_pred == Y_test) / len(y_pred))\n",
    "    acc = accuracy_score(Y_test, y_pred)\n",
    "    #acc = np.sum(y_pred == Y_test) / len(y_pred)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 3\n",
    "\n",
    "In this task, you need to obtain sentiment analysis for the provided dataset. The dataset consists of movie reviews with the sentiments being provided. The sentiments are either positive or negative. You need to train a boosting based classifier to obtain train and cross-validate on the dataset provided. The method will be evaluated against an external test set.\n",
    "\n",
    "This task is worth 25 points out of 100 points. 15 points will be for implementing the pre-processing and Bag of Words based feature extractor correctly and evaluating the boosting based classifier for the text features and validating it by cross-validation on the training set. 5 points are based on the evaluation carried out on a separate test dataset that will be done at the time of evaluation. Finally 5 points are reserved for analysis of this part of the task and presenting it well in a lab report.\n",
    "\n",
    "Use the movie_review_train.csv file provided with the assignment, and save it in the same directory as the Python notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Process the text and obtain a bag of words-based features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bag_of_words(df, \n",
    "                 max_features = 4000, \n",
    "                 min_df = 0.05):\n",
    "    \n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "    from sklearn.feature_extraction.text import TfidfTransformer \n",
    "    \n",
    "    vectorizer = CountVectorizer(stop_words = 'english', \n",
    "                                 min_df = min_df,\n",
    "                                 max_features = int(max_features))\n",
    "    x = vectorizer.fit_transform(df.iloc[:,-2])\n",
    "\n",
    "    transformer = TfidfTransformer(sublinear_tf = False)\n",
    "    tf = transformer.fit_transform(x)\n",
    "\n",
    "    df_tf = pd.DataFrame(tf.toarray())\n",
    "    df_tf['y'] = df.iloc[:,-1]\n",
    "    df_tf['y'].replace('positive', 1, inplace = True)\n",
    "    df_tf['y'].replace('negative', 0, inplace = True)\n",
    "    \n",
    "    return df_tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_bag_of_words_train(train_file):\n",
    "    \n",
    "    print('Extracting train text data.')\n",
    "    \n",
    "    # import dependancies\n",
    "    import nltk\n",
    "    from nltk.tokenize import word_tokenize\n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "    import re\n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "    from sklearn.feature_extraction.text import TfidfTransformer \n",
    "    import pandas as pd\n",
    "    \n",
    "    df_movies = pd.read_csv(train_file)\n",
    "    print('raw file shape:', df_movies.shape)\n",
    "    \n",
    "    df = bag_of_words(df_movies, \n",
    "                      max_features = 3000, # 3000\n",
    "                      min_df = 1)\n",
    "    print('Bag of words extraction: ', df.shape)\n",
    "    \n",
    "    df = reduce_features(df, num_features = 200)\n",
    "    print('PCA: ', df.shape)\n",
    "    \n",
    "    X, y = df.iloc[:,:-1], df.iloc[:,-1]\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_bag_of_words_test(test_file):\n",
    "    \n",
    "    print('Extracting test text data.')\n",
    "    \n",
    "    # import dependancies\n",
    "    import nltk\n",
    "    from nltk.tokenize import word_tokenize\n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "    import re\n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "    from sklearn.feature_extraction.text import TfidfTransformer \n",
    "    import pandas as pd\n",
    "    \n",
    "    df_movies = pd.read_csv(test_file)\n",
    "    print('raw file shape:', df_movies.shape)\n",
    "    \n",
    "    df = bag_of_words(df_movies, \n",
    "                      max_features = 3000,\n",
    "                      min_df = 1)\n",
    "    print('Bag of words extraction: ', df.shape)\n",
    "    \n",
    "    df = reduce_features(df, num_features = 200)\n",
    "    print('PCA: ', df.shape)\n",
    "    \n",
    "    X, y = df.iloc[:,:-1], df.iloc[:,-1]\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test function that will be called to evaluate your code. Separate train and test dataset will be provided\n",
    "\n",
    "Do not modify the code below. Please write your code above such that it can be evaluated by the function below. You can modify your code above such that you obtain the best performance through this function. We will also be evaluating the cross-validation performance with a set train and val split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_func_boosting_text(text_dataset_train, text_dataset_test):\n",
    "    from sklearn.metrics import accuracy_score    \n",
    "    (X_train, Y_train) = extract_bag_of_words_train(text_dataset_train)\n",
    "    (X_test, Y_test) = extract_bag_of_words_test(text_dataset_test) # optionally the two calls can be replaced by a single extract_bag_of_words_train_test() function\n",
    "    bc = BoostingClassifier()\n",
    "    bc.fit(X_train, Y_train)\n",
    "    y_pred = bc.predict(X_test)    \n",
    "    acc = accuracy_score(Y_test, y_pred)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 4\n",
    "\n",
    "In this task, you need to classify the above movie review dataset using a Support Vector Machine (SVM).\n",
    "\n",
    "This task is worth 20 points out of 100 points. You are allowed to use existing library functions such as scikit-learn for obtaining the SVM. The main idea is to analyse the dataset using different kind of kernels. You are also supposed to write your own custom text kernels. The marking will be 10 marks for analysing the dataset using various kernels including your own kernels, 5 points for the performance on the test dataset and 5 points for a lab-report that provides the analysis and comparisons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test function that will be called to evaluate your code. Separate train and test dataset will be provided\n",
    "\n",
    "Do not modify the code below. Please write your code above such that it can be evaluated by the function below. You can modify your code above such that you obtain the best performance through this function. We will also be evaluating the cross-validation performance with a set train and val split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_func_svm_text(text_dataset_train, text_dataset_test):\n",
    "    from sklearn.metrics import accuracy_score    \n",
    "    (X_train, Y_train) = extract_bag_of_words_train(text_dataset_train)\n",
    "    (X_test, Y_test) = extract_bag_of_words_test(text_dataset_test) # optionally the two calls can be replaced by a single extract_bag_of_words_train_test() function\n",
    "    sc = SVMClassifier()\n",
    "    sc.fit_text(X_train, Y_train)\n",
    "    y_pred = sc.predict_text(X_test)\n",
    "    acc = accuracy_score(Y_test, y_pred)\n",
    "    return acc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
